{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttMLpgHe0qS0"
      },
      "source": [
        "# BERT(Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "* 참고: https://ebbnflow.tistory.com/151\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9UVaxPPBXup"
      },
      "source": [
        "## Input Representation\n",
        "\n",
        "* 3가지의 입력 임베딩(Token, Segment, Position 임베딩)의 합으로 구성\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbABsUL%2FbtqzmTU7OLm%2FYwK6JLhNfTYvxkiFzkfkCK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BZpgYyoBlqW"
      },
      "source": [
        "### Token Embeddings\n",
        "\n",
        "* Word Piece 임베딩 방식 사용\n",
        "* 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 생성\n",
        "* 즉, 자주 등장하는 sub-word은 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 sub-word로 쪼개짐\n",
        "* 기존 워드 임베딩 방법은 Out-of-vocabulary (OOV) 문제가 존재하며, 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있음\n",
        "* Word Piece 임베딩은 모든 언어에 적용 가능하며, sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도 상승효과도 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Xa5v1iCBzE"
      },
      "source": [
        "### Sentence Embeddings\n",
        "\n",
        "* BERT는 두 개의 문장을 문장 구분자([SEP])와 함께 결합\n",
        "* 입력 길이의 제한으로 두 문장은 합쳐서 512 subword 이하로 제한\n",
        "* 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문에 적절한 입력 길이 설정 필요\n",
        "* 한국어는 보통 평균 20 subword로 구성되고 99%가 60 subword를 넘지 않기 때문에 입력 길이를 두 문장이 합쳐 128로 해도 충분\n",
        "* 간혹 긴 문장이 있으므로 우선 입력 길이 128로 제한하고 학습한 후, 128보다 긴 입력들을 모아 마지막에 따로 추가 학습하는 방식을 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dloBbq6CD7v"
      },
      "source": [
        "### Position Embedding\n",
        "\n",
        "* BERT는 저자의 이전 논문인 Transformer 모델을 착용\n",
        "* Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용\n",
        "* Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보가 필요\n",
        "* Transformer 에서는 Sinusoid 함수를 이용한 Positional encoding을 사용하였고, BERT에서는 이를 변형하여 Position encoding을 사용\n",
        "* Position encoding은 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-JatE-CF7P"
      },
      "source": [
        "### 임베딩 취합\n",
        "\n",
        "* BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 하나의 임베딩 값으로 생성\n",
        "* 임베딩의 합에 Layer Normalization과 Dropout을 적용하여 입력으로 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWYtaq0qCq6P"
      },
      "source": [
        "## 언어 모델링 구조(Pre-training BERT)\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbg5SlP%2FbtqzntBU7Uj%2FKHWiKI4zKgb8FqLzAYAusK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qVY1sl5C8dD"
      },
      "source": [
        "### 언어 모델링 데이터\n",
        "\n",
        "* BERT는 총 3.3억 단어(8억 단어의 BookCorpus 데이터와 25억 단어의 Wikipedia 데이터)의 거대한 말뭉치를 이용하여 학습\n",
        "* 거대한 말뭉치를 MLM, NSP 모델 적용을 위해 스스로 라벨을 만들고 수행하므로 준지도학습(Semi-supervised)이라고 함\n",
        "* Wikipedia와 BookCorpus를 정제하기 위해 list, table, header를 제거\n",
        "* 문장의 순서를 고려해야 하므로 문단 단위로 분리하였고 많은 데이터 정제 작업을 수행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTASWRoyDHKy"
      },
      "source": [
        "### 모델 구조\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbL28Ok%2FbtqznO6UmYw%2Fe0mFyA814Pvj4kltVxKls0%2Fimg.png)\n",
        "\n",
        "\n",
        "* BERT 모델은 Transformer를 기반으로 함\n",
        "* Transformer 모델 구조는 인코더-디코더 모델이며 번역 도메인에서 최고 성능을 기록\n",
        "* 기존 인코더-디코더 모델들과 다르게 Transformer는 CNN, RNN을 이용하지 않고 Self-attention이라는 개념을 도입\n",
        "* BERT는 Transformer의 인코더-디코더 중 인코더만 사용하는 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq2CX1-sDZWz"
      },
      "source": [
        "### MLM(Masked Language Model)\n",
        "\n",
        "* 입력 문장에서 임의로 Token을 마스킹(masking), 그 Token을 맞추는 방식인 MLM 학습 진행\n",
        "* 문장의 빈칸 채우기 문제를 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKUJCOJEDIN"
      },
      "source": [
        "* 생성 모델 계열은(예를들어 GPT) 입력의 다음 단어를 예측\n",
        "* MLM은 문장 내 랜덤한 단어를 마스킹 하고 이를 예측\n",
        "* 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹\n",
        "* 이 때 80%는 [MASK]로 바꿔주지만, 나머지 10%는 다른 랜덤 단어로, 또 남은 10%는 바꾸지 않고 그대로 둠\n",
        "* 이는 미세 조정 시 올바른 예측을 돕도록 마스킹에 노이즈를 섞음\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLMyXN%2Fbtqzl4Ql7sH%2FykzRZNWkc6rcb8ffU5Nrm1%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYxZ3AV5EMt5"
      },
      "source": [
        "* 아래 그림은 MLM의 학습 과정\n",
        "* 입력 단어의 15%가 [MASK]로 대체된 입력이 들어가고, MLM은 [MASK]가 어떤 단어인지를 예측\n",
        "* BERT의 Token 임베딩은 Word Piece 임베딩 방식을 사용하고, Word piece의 단어수는 30522 단어\n",
        "* 3만 단어 중 [MASK]에 들어갈 단어를 찾는 것이므로 MLM의 출력인 Softmax의 클래스는 3만개\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc0lfDS%2FbtqzmTOp4JK%2FXkDq157Mw7MnycHeC2NAx1%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbIouOAAEW3P"
      },
      "source": [
        "### NSP(Next Sentence Prediction)\n",
        "\n",
        "* NSP는 두 문장이 주어졌을 때 두 번째 문장이 첫 번째 문장의 바로 다음에 오는 문장인지 여부를 예측하는 방식\n",
        "* 두 문장 간 관련이 고려되어야 하는 NLI와 QA의 파인튜닝을 위해 두 문장이 연관이 있는지를 맞추도록 학습\n",
        "* 아래 그림은 NSP의 입력 예시\n",
        "* 위에서 설명한 MLM과 동시에 NSP도 적용된 문장들\n",
        "* 첫 번째 문장과 두 번째 문장은 [SEP]로 구분\n",
        "* 두 문장이 실제로 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 상관 없는 문장으로 구성\n",
        "* 이 학습을 통해 문맥과 순서를 언어모델이 학습 가능\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmRPzz%2Fbtqzps28Eyd%2F2ak5AHBLlk1jXHnOgGwyMK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o2iHcTxEpPf"
      },
      "source": [
        "* 아래 그림은 NSP의 학습 방법\n",
        "* 연속 문장인지, 아닌지만 판단하면 되므로 Softmax의 출력은 2개이고 3만개의 출력을 갖는 MLM에 비해 빠르게 학습\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlyapH%2FbtqzmkrVtki%2FUUqjexLh7Lt4ZwMVpjIBJ1%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDPn-eM-Eycz"
      },
      "source": [
        "## 학습된 언어모델 전이학습(Transfer Learning)\n",
        "\n",
        "* 파인 튜닝은 학습된 언어 모델을 이용하여 실제 자연어처리 문제를 푸는 과정\n",
        "* 실질적으로 성능이 관찰되는 것은 전이학습 이지만, 언어 모델이 제대로 학습되야 전이학습 시 좋은 성능이 나옴\n",
        "* 기존 알고리즘들은 자연어의 다양한 Task에 각각의 알고리즘을 독립적으로 만들어야 했지만, BERT 개발 이후 많은 자연어처리 연구자들은 언어 모델을 만드는데 더 공을 들이게 됨\n",
        "* 전이학습 Task의 성능도 훨씬 더 좋아짐\n",
        "* 전이학습은 라벨이 주어지므로 지도학습(Supervised learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyfDPHbkFHgr"
      },
      "source": [
        "* 전이학습은 BERT의 언어 모델의 출력에 추가적인 모델을 쌓아서 사용\n",
        "* 일반적으로 복잡한 CNN, LSTM, Attention을 쌓지 않고 간단한 DNN만 쌓아도 성능이 잘 나오며 별 차이가 없다고 알려짐\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdHqgat%2Fbtqzl4CSqNd%2F7q3g5hxTcAENvvcu1wK6KK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i05t2LQaFjVS"
      },
      "source": [
        "## BERT 친구들\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMaiOM%2FbtqznO6UO3m%2FwvMAVAZDLngmplVbkn0gqK%2Fimg.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YNQLQHji8lM"
      },
      "source": [
        "# BERT 네이버 영화 리뷰 분류\n",
        "\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvIRqh0AIl8d"
      },
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiKkbJmvkd98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964a72ee-4950-4fd7-b28e-f1ceb2f652ea"
      },
      "source": [
        "!pip install transformers==2.11.0\n",
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.11.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[K     |████████████████████████████████| 674 kB 18.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 26.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (2022.6.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==2.11.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=13f87dc4cd0b7f9966406d297a5ac4757b408aa37c4b77c0f0192399e60c0586\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.7.0 transformers-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.48.1)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.21.6)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.14.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, scipy, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.15+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.17 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAd2vXBshrRh"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDRKt69ChrRY"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP74lUeSIABN"
      },
      "source": [
        "tf.random.set_seed(111)\n",
        "np.random.seed(111)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN = 39"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OboRn3Vcj_GW"
      },
      "source": [
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_eD3MbDphrSB"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "train_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "test_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
        "\n",
        "train_data = pd.read_table(train_file)\n",
        "test_data = pd.read_table(test_file)\n",
        "\n",
        "train_data = train_data.dropna()\n",
        "test_data = test_data.dropna()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ZCDWgskiRp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cb04b9e7-16a2-4e44-9d1f-5554f1c74da3"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00fea69c-2483-490d-9cd7-9683cee7c233\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00fea69c-2483-490d-9cd7-9683cee7c233')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00fea69c-2483-490d-9cd7-9683cee7c233 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00fea69c-2483-490d-9cd7-9683cee7c233');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVnAFFU-kiny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "711f96ce-8627-4a3f-e180-2979b2b48967"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                           document  label\n",
              "0  6270596                                                굳 ㅋ      1\n",
              "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
              "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
              "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5dc31e3-c225-40d9-8b68-03d0df0a4655\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td>GDNTOPCLASSINTHECLUB</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5dc31e3-c225-40d9-8b68-03d0df0a4655')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5dc31e3-c225-40d9-8b68-03d0df0a4655 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5dc31e3-c225-40d9-8b68-03d0df0a4655');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbG9rFUZkoXv"
      },
      "source": [
        "## BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYk5cINxlIcM"
      },
      "source": [
        "* 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymur-MI3hrSJ"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)\n",
        "\n",
        "\n",
        "def bert_tokenizer(sentence, MAX_LEN):\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text = sentence,\n",
        "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True   # Construct attn. masks.\n",
        "        \n",
        "    )\n",
        "    \n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
        "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tagwY491hrSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d990a0-96bd-4271-bb6a-c128b22d02a0"
      },
      "source": [
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "train_data_labels = []\n",
        "\n",
        "for train_sentence, train_label in tqdm(zip(train_data[\"document\"], train_data[\"label\"]), total=len(train_data)):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sentence, MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        train_data_labels.append(train_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(train_sentence)\n",
        "        pass\n",
        "\n",
        "train_movie_input_ids = np.array(input_ids, dtype=int)\n",
        "train_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
        "train_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
        "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
        "\n",
        "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
        "\n",
        "print(\"# sentences: {}\\n# labels: {}\".format(len(train_movie_input_ids), len(train_data_labels)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149995/149995 [00:38<00:00, 3897.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sentences: 149995\n",
            "# labels: 149995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQJguadKhrSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc91744-98c3-444c-c624-ceb84102d9af"
      },
      "source": [
        "idx = 5\n",
        "\n",
        "input_id = train_movie_input_ids[idx]\n",
        "attention_mask = train_movie_attention_masks[idx]\n",
        "token_type_id = train_movie_type_ids[idx]\n",
        "\n",
        "\n",
        "print(input_id)\n",
        "print(attention_mask)\n",
        "print(token_type_id)\n",
        "print(tokenizer.decode(input_id))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   101   9247   8867  32158  23811    100    124  24982  17655   9757\n",
            "  55511    122  23321  10954  24017  12030    129 106249  24974  30858\n",
            "  18227    119    100    119    119    119   9353  30134  21789  12092\n",
            "   9519 118671 119169    119    102      0      0      0      0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0]\n",
            "[CLS] 막 걸음마 [UNK] 3세부터 초등학교 1학년생인 8살용영화. [UNK]... 별반개도 아까움. [SEP] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaO33a6ChrSW"
      },
      "source": [
        "class TFBertClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1] \n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfubbrJBxhDa"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df3MZ75XhrSa"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBygAVGhrSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf27ddaf-df28-4db1-b7dd-27a12935fa09"
      },
      "source": [
        "model_name = \"tf2_bert_naver_movie\"\n",
        "\n",
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "checkpoint_path = os.path.join('./', model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습과 eval 시작\n",
        "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
        "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
        "\n",
        "#steps_for_epoch\n",
        "\n",
        "print(history.history)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./tf2_bert_naver_movie -- Folder already exists \n",
            "\n",
            "Epoch 1/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.4073 - accuracy: 0.8105\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.84743, saving model to ./tf2_bert_naver_movie/weights.h5\n",
            "3750/3750 [==============================] - 1233s 329ms/step - loss: 0.4073 - accuracy: 0.8105 - val_loss: 0.3486 - val_accuracy: 0.8474\n",
            "Epoch 2/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.8614\n",
            "Epoch 00002: val_accuracy improved from 0.84743 to 0.85506, saving model to ./tf2_bert_naver_movie/weights.h5\n",
            "3750/3750 [==============================] - 1230s 328ms/step - loss: 0.3207 - accuracy: 0.8614 - val_loss: 0.3376 - val_accuracy: 0.8551\n",
            "Epoch 3/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.8889\n",
            "Epoch 00003: val_accuracy did not improve from 0.85506\n",
            "3750/3750 [==============================] - 1232s 329ms/step - loss: 0.2659 - accuracy: 0.8889 - val_loss: 0.3412 - val_accuracy: 0.8541\n",
            "Epoch 4/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9136\n",
            "Epoch 00004: val_accuracy improved from 0.85506 to 0.85910, saving model to ./tf2_bert_naver_movie/weights.h5\n",
            "3750/3750 [==============================] - 1235s 329ms/step - loss: 0.2142 - accuracy: 0.9136 - val_loss: 0.3733 - val_accuracy: 0.8591\n",
            "Epoch 5/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9311\n",
            "Epoch 00005: val_accuracy improved from 0.85910 to 0.85946, saving model to ./tf2_bert_naver_movie/weights.h5\n",
            "3750/3750 [==============================] - 1232s 329ms/step - loss: 0.1754 - accuracy: 0.9311 - val_loss: 0.3723 - val_accuracy: 0.8595\n",
            "Epoch 6/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9448\n",
            "Epoch 00006: val_accuracy improved from 0.85946 to 0.86043, saving model to ./tf2_bert_naver_movie/weights.h5\n",
            "3750/3750 [==============================] - 1232s 328ms/step - loss: 0.1423 - accuracy: 0.9448 - val_loss: 0.4151 - val_accuracy: 0.8604\n",
            "Epoch 7/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9544\n",
            "Epoch 00007: val_accuracy did not improve from 0.86043\n",
            "3750/3750 [==============================] - 1229s 328ms/step - loss: 0.1191 - accuracy: 0.9544 - val_loss: 0.4689 - val_accuracy: 0.8586\n",
            "Epoch 8/10\n",
            "3750/3750 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9603\n",
            "Epoch 00008: val_accuracy did not improve from 0.86043\n",
            "3750/3750 [==============================] - 1229s 328ms/step - loss: 0.1046 - accuracy: 0.9603 - val_loss: 0.5118 - val_accuracy: 0.8603\n",
            "{'loss': [0.4072941839694977, 0.3206850588321686, 0.2659023106098175, 0.21416273713111877, 0.17541325092315674, 0.14229610562324524, 0.11909592151641846, 0.10460135340690613], 'accuracy': [0.8104686737060547, 0.8614037036895752, 0.8889129757881165, 0.9135887622833252, 0.9311310648918152, 0.944823145866394, 0.9544067978858948, 0.9603486657142639], 'val_loss': [0.34860801696777344, 0.3376275897026062, 0.34121736884117126, 0.3733386695384979, 0.3723333775997162, 0.41507235169410706, 0.4688817858695984, 0.5118193626403809], 'val_accuracy': [0.8474282622337341, 0.8550618290901184, 0.8540951609611511, 0.8590952754020691, 0.8594619631767273, 0.8604286909103394, 0.8585619330406189, 0.8602620363235474]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzUB0CmvhrSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ca23d93b-1b9e-4321-ab27-6a99f26ba3a1"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'], '')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hVVfb/8fdKDwRCCy2FGnon9CaIiA1URikWxIK961cdnZ8O44yOY+8iImJDFHUYGzoo0pEQeg89ASGEGiB9/f44FwyZEALk5tybrNfz5Mk97d4VHs0nZ+999hZVxRhjjCkswO0CjDHG+CYLCGOMMUWygDDGGFMkCwhjjDFFsoAwxhhTpCC3CygttWrV0oYNG7pdhjHG+JUlS5bsVdWooo6Vm4Bo2LAhiYmJbpdhjDF+RUS2neqYNTEZY4wpkgWEMcaYIllAGGOMKVK56YMoSk5ODikpKWRmZrpdis8LCwsjJiaG4OBgt0sxxviIch0QKSkpVKlShYYNGyIibpfjs1SV9PR0UlJSaNSokdvlGGN8RLluYsrMzKRmzZoWDqchItSsWdPutIwxJynXAQFYOJSQ/TsZYwor9wFhjDHlVn4erJoGSyZ55e3LdR+EL4iIiCAjI8PtMowx5UleDqz4DOa+BOnJENMFOo2GUm4JsIAwxhh/kXMMkj6E+a/CwR1Qty1c9QG0vKzUwwGsianMqCoPP/wwbdq0oW3btnz22WcA7Nq1i759+9KhQwfatGnDnDlzyMvL44Ybbjhx7ksvveRy9cYYV2Uegrkvw8vt4PuHoWp9GPU53DoHWl8OAYFe+Viv3kGIyGDgFSAQmKCqzxY6fgPwLyDVs+t1VZ3gOTYaeMKz/2lV/eBcavnrf1azZuehc3mL/9GqflWevKx1ic798ssvWbZsGcuXL2fv3r106dKFvn378sknn3DhhRfy+OOPk5eXx9GjR1m2bBmpqamsWrUKgAMHDpRq3cYYP3F0Hyx62/nKPAiN+0Pf96FBL6/cMRTmtYAQkUDgDeACIAVYLCLTVXVNoVM/U9W7Cl1bA3gSSAAUWOK5dr+36vW2uXPnMnLkSAIDA6lTpw79+vVj8eLFdOnShRtvvJGcnBwuv/xyOnToQOPGjdm8eTN33303l1xyCYMGDXK7fGNMWTr8Oyx4HRZPhJwj0OJS6PMARHcu0zK8eQfRFUhW1c0AIjIFGAoUDoiiXAj8pKr7PNf+BAwGPj3bYkr6l35Z69u3L7Nnz+bbb7/lhhtu4IEHHuD6669n+fLlzJgxg7fffpupU6cyceJEt0s1xnjb/m0w7xVY+hHk50CbYdD7AajTypVyvNkHEQ3sKLCd4tlX2DARWSEiX4hI7JlcKyJjRSRRRBLT0tJKq26v6NOnD5999hl5eXmkpaUxe/ZsunbtyrZt26hTpw633HILN998M0lJSezdu5f8/HyGDRvG008/TVJSktvlG2O8KW0DfHUbvNoRkiZD+xFwVyIMm+BaOID7o5j+A3yqqlkicivwATCgpBer6nhgPEBCQoJ6p8TSccUVV7BgwQLat2+PiPDcc89Rt25dPvjgA/71r38RHBxMREQEkydPJjU1lTFjxpCfnw/AM88843L1xhiv2LUc5rwAa6ZDUBh0HQs974bIov6WLnui6p3fqyLSA3hKVS/0bD8GoKpF/rbz9FnsU9VIERkJnKeqt3qOvQPMUtVTNjElJCRo4QWD1q5dS8uWLUvl56kI7N/LmDKyfSHMfh6Sf4LQqtD1Fuh+B1SuVealiMgSVU0o6pg37yAWA/Ei0ghnlNIIYFShwuqp6i7P5hBgref1DOAfIlLdsz0IeMyLtRpjjHepwuZfYPYLsG0uVKoJA/4CXW6G8GpuV1ckrwWEquaKyF04v+wDgYmqulpExgGJqjoduEdEhgC5wD7gBs+1+0TkbzghAzDueIe1Mcb4lfx8WP+d05S0Mwmq1IcLn4HOoyGkstvVFcurfRCq+h3wXaF9/6/A68c4xZ2Bqk4EbOiOMcY/5eXC6i9hzouQthaqN4TLXoH2IyEo1O3qSsTtTmpjjClfcrNg+afOk8/7t0BUS7hyArS+AgL961euf1VrjDG+KvsILPkA5r8Gh3dC/Y4w6GNofjEE+OesRhYQxhhzLjIPwm/vwsI34Wg6NOgNl7/hTIvh5+usWEAYY8zZOLLXCYXf3oWsQ9D0Auj7EMR1d7uyUmMB4WOKWz9i69atXHrppScm8TPGuOBgqjNP0pJJzvTbrYZAnwehXnu3Kyt1FhDGGFMS+zY7Hc/LPgHNh3bDofd9ENXc7cq8puIExPePwu8rS/c967aFi54t9pRHH32U2NhY7rzzTgCeeuopgoKC+OWXX9i/fz85OTk8/fTTDB069Iw+OjMzk9tvv53ExESCgoJ48cUX6d+/P6tXr2bMmDFkZ2eTn5/PtGnTqF+/PldffTUpKSnk5eXxl7/8heHDh5/1j21MhbJnrTNUddUXEBDsPL/Q8x6o3sDtyryu4gSES4YPH8599913IiCmTp3KjBkzuOeee6hatSp79+6le/fuDBkyBDmDDq033ngDEWHlypWsW7eOQYMGsWHDBt5++23uvfderrnmGrKzs8nLy+O7776jfv36fPvttwAcPHjQKz+rMeVKapLzcNu6byC4MvS4E3rcBVXqul1Zmak4AXGav/S9pWPHjuzZs4edO3eSlpZG9erVqVu3Lvfffz+zZ88mICCA1NRUdu/eTd26Jf8Pb+7cudx9990AtGjRggYNGrBhwwZ69OjB3//+d1JSUrjyyiuJj4+nbdu2PPjggzzyyCNceuml9OnTx1s/rjH+b/dq+PEvsGkmhFWDfo9Ct1uhUg23Kytz/jk4189cddVVfPHFF3z22WcMHz6cjz/+mLS0NJYsWcKyZcuoU6cOmZmZpfJZo0aNYvr06YSHh3PxxRfz888/06xZM5KSkmjbti1PPPEE48aNK5XPMqZcyTwIPzwGb/eBnUth4F/h/lXQ/7EKGQ5Qke4gXDR8+HBuueUW9u7dy6+//srUqVOpXbs2wcHB/PLLL2zbtu2M37NPnz58/PHHDBgwgA0bNrB9+3aaN2/O5s2bady4Mffccw/bt29nxYoVtGjRgho1anDttddSrVo1JkyY4IWf0hg/pQorP4cfn4CMPZBwIwx4osKGQkEWEGWgdevWHD58mOjoaOrVq8c111zDZZddRtu2bUlISKBFixZn/J533HEHt99+O23btiUoKIhJkyYRGhrK1KlT+fDDDwkODqZu3br8+c9/ZvHixTz88MMEBAQQHBzMW2+95YWf0hg/tHsNfPcQbJvnLOc5cgpEd3K7Kp/htfUgypqtB3Hu7N/LVBiZh2DWs7DobQiLhIFPQcfr/HZKjHPh1noQxhjjWwo3J3W+Ac7/f9acdAoWED5o5cqVXHfddSftCw0NZdGiRS5VZEw5ULA5qX4na04qAa8GhIgMBl7BWTBogqoWOdZURIYBXwBdVDVRRBrirC633nPKQlW97WxqUNUzer7AF7Rt25Zly5aV6WeWl6ZGY/5H5iH49Z+w8C0Iq+qsydDx+grZnHSmvBYQnjWm3wAuAFKAxSIyXVXXFDqvCnAvUPjP402q2uFcaggLCyM9PZ2aNWv6XUiUJVUlPT2dsLAwt0sxpvSowsovPM1Ju50noM9/0pqTzoA37yC6AsmquhlARKYAQ4E1hc77G/BP4OHSLiAmJoaUlBTS0tJK+63LnbCwMGJiYtwuw5jSsWctfPuQs/Zz/Y4w4hOI6ex2VX7HmwERDewosJ0CdCt4goh0AmJV9VsRKRwQjURkKXAIeEJV5xT+ABEZC4wFiIuL+58CgoODadSo0Tn9EMYYP5J1+I/RSaFV4NKXodP1EBDodmV+ybVOahEJAF4Ebiji8C4gTlXTRaQz8LWItFbVQwVPUtXxwHhwhrl6uWRjjK9ShVXTnOakw787oXD+k1C5ptuV+TVvBkQqEFtgO8az77gqQBtglqd/oC4wXUSGqGoikAWgqktEZBPQDDj5QQdjjNmzFr57GLbOcZqThn9szUmlxJsBsRiIF5FGOMEwAhh1/KCqHgRqHd8WkVnAQ55RTFHAPlXNE5HGQDyw2Yu1GmP8TdbhP0YnhUTApS9Bp9HWnFSKvBYQqporIncBM3CGuU5U1dUiMg5IVNXpxVzeFxgnIjlAPnCbqu7zVq3GGD9yUnPSLk9z0lPWnOQF5XqqDWNMObNnnfOw29Y5UK8DXPICxBQ5S4QpIZtqwxjj37IOw6/PwcI3neakS150psmw5iSvsoAwxvguVVj9Jcx4Ag7vdCbUG/gUVK51uitNKbCAMMb4prT1TnPSltlQtx1cPRliu7hdVYViAWGM8S1ZGTD7OVjwBoRUdvoZOo+x5iQXWEAYY3yDKqz+CmY87mlOutZZ9tOak1xjAWGMcV/aBk9z0q9Qty1c/QHEdnW7qgrPAsIY456sDJj9L09zUiW4+HlnTWhrTvIJFhDGmLKnCmu+dpqTDqVCh2ud0UkRUW5XZgqwgDDGlK20DfD9w7B5ltOc9Kf3Ia7baS8zZc8CwhhTNrKPOM1J81+HYGtO8gcWEMYY78rLdeZOmjkODqVAh2uc0UnWnOTzLCCMMd6RmwXLPoF5L8P+rZ7mpPcgrrvblZkSsoAwxpSurAxYMgkWvO7MthrdGS78BzS7CAIC3K7OnAELCCA/XwkIELfLMMa/HdsPi8bDorec1436whVvQ6N+IPb/lz+q8AGRnpHFDe8v5sFBzTiveW23yzHG/xzeDQvfgMXvQXYGNL8Yej9g8yaVA1693xORwSKyXkSSReTRYs4bJiIqIgkF9j3muW69iFzorRoDRFCUWyYn8uPq3731McaUP/u3wbcPwsttYf5r0PwiuH0+jPzUwqGc8FpAiEgg8AZwEdAKGCkirYo4rwpwL7CowL5WOEuUtgYGA2963q/UVa8cwsc3d6d1/Uju+DiJb1bs9MbHGFN+pK2Hr26DVzvCkg+g/Qi4KxGGTYA6rd2uzpQib95BdAWSVXWzqmYDU4ChRZz3N+CfQGaBfUOBKaqapapbgGTP+3lFZHgwH93cjU5x1bnn06V8mZTirY8yxn/tXAqfXQtvdIM1/4Zut8G9y2HIq1CzidvVGS/wZh9ENLCjwHYKcNLjkiLSCYhV1W9F5OFC1y4sdG20twoFiAgNYtKNXRg7eQkPfr6crNx8RnaN8+ZHGuP7VGHbfJjzAmyaCaGR0Pch6Ha7rQFdAbjWSS0iAcCLwA3n8B5jgbEAcXHn/su8UkgQE0YncPtHS3jsy5Vk5+YzumfDc35fY/yOKmz8yQmGHQuhcpQzV1LCTRBW1e3qTBnxZkCkArEFtmM8+46rArQBZokzBK4uMF1EhpTgWgBUdTwwHiAhIUFLo+iw4EDevq4zd3+ylCenryYrN4+xfe322VQQ+XmwdroTDL+vhMhYZ0qMjtdCcLjb1Zky5s2AWAzEi0gjnF/uI4BRxw+q6kHgxEogIjILeEhVE0XkGPCJiLwI1Afigd+8WOtJQoMCeeOaTjwwdTn/+G4dWTn53H1+fFl9vDFlLzcbVk6FuS9BejLUjIfL34K2V0FgsNvVGZd4LSBUNVdE7gJmAIHARFVdLSLjgERVnV7MtatFZCqwBsgF7lTVPG/VWpTgwABeHt6BkMAAXvhpA5m5eTw0qDliD/yY8iT7KCz9EOa96syTVLcdXPUBtLzMJtEziGqptMy4LiEhQRMTE0v9ffPzlce/XsWnv23npt6NeOKSlhYSxv9lHoTFE2DBm3B0L8T1hD4PQtPz7annCkZElqhqQlHHKvyT1KcTECD844o2hAYF8N7cLWTl5jFuSBubmsP4pyN7YeFb8Nu7kHUQmg50gqFBT7crMz7IAqIERIQnL2vldGD/uons3HyeubIdgRYSxl8cTHWedl4yCXIzodUQZzqM+h3crsz4MAuIEhIRHhncnLDgAF7+70aycvN54ar2BAXa7JTGh6VvcqbbXvYpoNBuOPS6D6KauV2Z8QMWEGdARLhvYDNCggJ47of1ZOfm88qIjoQEWUgYH/P7Kpj7Iqz+CgJDIGEM9LwbqtnDn6bkLCDOwh3nNSU0KJC/fbOG7I+W8MY1nQgLthEfxgfs+M15hmHDDxBSBXreAz3uhAibqdicOQuIs3RT70aEBgXwxNeruGVyIuOvSyA8xELCuEAVNs9ygmHrHAivAf2fgK43Q3h1t6szfswC4hxc270BoUEBPDJtBWMm/cZ7o7tQOdT+Sf3Wge2QtsEZ5hkQCBL4x3cJcFZDK7jvxPeAAucUPFZ4u+C+UhjgkJ8P679zgmFnElSpDxc+A51HQ0jlc39/U+HZb7NzdFVCLCFBATwwdTnXvbeISTd2pWqYPXnqN47uc2YmXTEVts8vww8uIoRKHDSe8zIPwsHtUL0RXPaqM+12UGgZ/gymvLOAKAVDO0QTGhTA3Z8u5doJi5h8Y1eqVQpxuyxzKjmZsHGGEwobZkB+DtRqDgP+Ag17O+dovjMvkeZ5vhfePr4v/+R9J77nO8f/51gR73vi/DN8H2kAA5+EVpdDoP2vbEqf/VdVSga3qcc71wVw20dJjHx3ER/d1JWaEfbXnM/Iz4dt85z5hlb/23lILKIOdLsV2l3tTDFhTxAbcxKbaqOUzd24l5snLyameiU+ubkbtauGuV1SxbZ7Daz4DFZ+4cw1FBLhzDPU7mpo1M/mGzIVXnFTbVhAeMGizencOGkxUVVC+eSW7tSvZtMkl6mDqbDqC1jxOexe6bTXNx3ohELziyGkktsVGuMzLCBckLR9P6Mn/kZkeDCf3tKd2Br2S8mrMg/C2v84dwtb5gAK0QnOk8NtroTKtU77FsZURBYQLlmZcpDrJi4iPDiQj2/uRuOoCLdLKl9ysyH5v06/wvrvnTmGajR2QqHtVbZOsjElYAHhorW7DnHthEUEBAif3NyN+DpV3C7Jv6k6Twuv+AxWfwnH9kOlWs5dQrvhEN3ZOpuNOQMWEC5L3nOYUe8uIjdf+fCmrrSuH+l2Sf4nbYNzp7BiKhzYBkHh0OISJxSa9LdVz4w5S64FhIgMBl7BWVFugqo+W+j4bcCdQB6QAYxV1TUi0hBYC6z3nLpQVW8r7rN8OSAAtu49wqh3F3IkO4/JN3alfWw1t0vyfRl7YNU0525h51LnAbHG50Hbq6HlpRBqd2PGnCtXAkJEAoENwAVACs4a1SNVdU2Bc6qq6iHP6yHAHao62BMQ36hqm5J+nq8HBMCOfUe5ZsIi9h/J5v0xXUhoWMPtknxPVgas+9a5W9j0i/NgWL32ns7mYVClrtsVGlOuuLWiXFcgWVU3e4qYAgzFWWcagOPh4FEZKPv2LlVnPpvIWKgWC2HVvNaGHVujElNv7cGodxdy/URn7qYeTWp65bP8Sl6uM9ncis9g3TeQcxQi46D3/c7Q1KjmbldoTIXkzYCIBnYU2E4BuhU+SUTuBB4AQoABBQ41EpGlwCHgCVWdU8S1Y4GxAHFxZznPfcYemDLqj+3Qqn+ERbW4Qq/jnOGS5xAgdSPDmHJrd66dsIgb3v+N8dcn0K9Z1Fm/n99SdSaYWzHVaUY6kuaEc7vhzldsN2duImOMa7zZxPQnYLCq3uzZvg7opqp3neL8UcCFqjpaREKBCFVNF5HOwNdA60J3HCc56yam3GznYaoDO5zZPA/uOPl1VqGPDAp3AiPSExrVYqFagz+CJKJuiX6x7TuSzbUTFpG8J4M3r+nEwFZ1zrx2f7RvC6z83LlbSE+GwFBoPtjpV4i/wCabM6aMudXElArEFtiO8ew7lSnAWwCqmgVkeV4vEZFNQDOg9DsZgkKcoZHRnYs+fuxAEcGx3Xm9cykc23fy+YEhUDX65LuO40ESGescCwyiRuUQPr2lO9e//xu3fbSEV0d25OK29Ur9x/MJR9KdIakrpkLKb4A4k+L1uhdaDoFw67A3xhd5MyAWA/Ei0ggnGEYAowqeICLxqrrRs3kJsNGzPwrYp6p5ItIYiAc2e7HWUwuv5nzVa1f08ayMP8Lj4HYnQA7scPZt/Akydp98vgRC1fpQLY7IyFimxkczMSuPTz9dQfCBvlzQo7Pv/BWdn+f0B+Qcc75nF3idcwxyjpy8nX305PNzjsKRvc4kefm5ULsVDPwrtP0TRMa4/dMZY07D28NcLwZexhnmOlFV/y4i44BEVZ0uIq8AA4EcYD9wl6quFpFhwDjP/nzgSVX9T3Gf5bOjmHIy4WDKH3cdhe9GDu90pm4uKKLuyXcd1eL++IqMdeYSys87+RfxaX+JFzx+tMC+Yn7J52Wd+c8bGALB4RBcyfkeEuEMTW03HOqWeFCaMaaM2INyviwvBw7tJDt9K5O+m0PG7i0MaZhD05D9njBJddYrKCgwBPKyz/yzAoKdcDn+yzu48OvwEhyvfPJ24XNsXQJj/IpbfRCmJAKDoXoDQqo3YPQdvbnrk6W8umY3T1zSkpv7NHbuFDJ2F2i62u70i5z0i/pUv8TDIbjAa3va2BhzBiwgfEhoUCBvXtOJ+z5bxtPfriUzJ4+7BsQ7fRZV60Ncd7dLNMZUIBYQPiY4MIBXhncgNDCA53/cQFZuPg9c0AyxCeiMMWXMAsIHBQUG8PxV7QkNDuC1n5PJys3nsYtaWEgYY8qUBYSPCggQ/nFFW0KDAhk/ezNZOXk8eVlrAgIsJIwxZcMCwoeJCE9e1orQoADemb2ZrNx8/n5FWwItJIwxZaBEASEilYFjqpovIs2AFsD3qppzmkvNORIRHr2oBaHBgbw6cyNZufn860/tCAq0eYqMMd5V0juI2UAfEakO/IjzlPRw4BpvFWb+ICI8cEEzQoMC+NeM9RzOzOWZK9sSVcVHnrg2xpRLJf0zVFT1KHAl8KaqXgW09l5Zpih39m/KX4e0ZvaGNAa8MIsPF2wlL798POhojPE9JQ4IEemBc8fwrWdfoHdKMsUZ3bMhP9zXh/Yx1fjLv1dzxZvzWJFywO2yjDHlUEkD4j7gMeArz1xJjYFfvFeWKU7jqAg+vKkrr43syO8HMxn6xjz+8vUqDh6zLiFjTOk547mYRCQAZ62GU67N4Aa/nYvpHB3KzOHFHzcwecFWalQO4fFLWnJ5h2h7ZsIYUyLFzcVUojsIEflERKp6RjOtAtaIyMOlWaQ5O1XDgnlqSGum39WbmOqVuP+z5Yx8dyHJew67XZoxxs+VtImpleeO4XLge6ARcJ3XqjJnrE10JF/e3pN/XNGWtbsOc9Erc3juh3Ucy85zuzRjjJ8qaUAEi0gwTkBM9zz/YMNnfExAgDCqWxwzH+zH0A7RvDlrEwNf/JX/rtl9+ouNMaaQkgbEO8BWoDIwW0QaAKftgxCRwSKyXkSSReTRIo7fJiIrRWSZiMwVkVYFjj3muW69iFxYwjoNUCsilOevas/UW3tQOTSQmycncsvkRFL2H3W7NGOMHznrBYNEJEhVc4s5HghsAC4AUnAerhupqmsKnFP1eGe3iAwB7lDVwZ6g+BToCtQH/gs0U9VTtpdU1E7q08nJy2fi3C28/F9nZdd7zo/npt6NCAmyJ7GNMaXTSR0pIi+KSKLn6wWcu4nidAWSVXWzqmYDU4ChBU8oNBKqMn80Ww0FpqhqlqpuAZI972fOUHBgALf2a8J/H+xH32a1+OcP67jk1Tks3JzudmnGGB9X0j8jJwKHgas9X4eA909zTTSwo8B2imffSUTkThHZBDwH3HOG1449HlppaWkl/FEqpuhq4bxzXQLvjU7gWE4eI8Yv5IGpy9ibcRbrThtjKoSSBkQTVX3SczewWVX/CjQujQJU9Q1VbQI8AjxxhteOV9UEVU2IiooqjXLKvfNb1uGn+/txV/+m/Gf5TgY8P4uPFm6zKTuMMf+jpAFxTER6H98QkV7AsdNckwrEFtiO8ew7lSk4o6TO5lpzBsJDAnnowuZ8f29f2kRH8sTXq7jyrfmsSj3odmnGGB9S0oC4DXhDRLaKyFbgdeDW01yzGIgXkUYiEgKMAKYXPEFE4gtsXgJs9LyeDowQkVARaQTEA7+VsFZTQk1rR/Dxzd14ZUQHUvcfY8jrc3ny36s4lGlTdhhjSjjdt6ouB9qLSFXP9iERuQ9YUcw1uSJyFzADZ2K/iZ55nMYBiao6HbhLRAYCOcB+YLTn2tUiMhVYA+QCdxY3gsmcPRFhaIdo+reozQsz1vPhwm18u/J3/nJpS4a0r29TdhhTgZ3LMNftqhpXyvWcNRvmWjpWphzkia9XsjzlIL2a1mTc0DY0iYpwuyxjjJec8zDXU73vOVxrfFTbmEi+vKMXf7u8DStSDjL45dk8P2O9TdlhTAV0LgFhw17KqcAA4bruDfj5wfO4rF19Xv8lmQte+pWf19mUHcZUJMUGhIgcFpFDRXwdxnnC2ZRjUVVCeXF4B6aM7U5YcCA3Tkrk1g8TST1wugFsxpjy4Kz7IHyN9UF4V3ZuPu/N3cIrMzcgCPcNjOfG3o0IDrQpO4zxZ97qgzAVSEhQALef14T/PtCP3vG1eOZ7Z8qO37bsc7s0Y4yXWECYMxJTvRLvXp/AhOsTOJKVx9XvLODBqctJtyk7jCl3LCDMWRnYqg4/PdCXO85rwvTlqQx44Vc+XrSNfJuyw5hywwLCnLVKIUH83+AWfH9vH1rWq8LjX9mUHcaUJxYQ5pw1rV2FT2/pzkvD25Oy/yhDXp/LU9NXc9im7DDGr1lAmFIhIlzRMYaZD5zHNd0a8MGCrZz/wq9MX76T8jJSzpiKxgLClKrISsH87fI2fH1HL+pUDeOeT5cy8t2FrNl52hVqjTE+xgLCeEX72Gp8faczZcf63w9z6WtzePyrlTbayRg/YgFhvOb4lB2zHurP6J4NmbJ4B/2fn8XEuVvIyct3uzxjzGlYQBivi6wUzJOXteaHe/vQPrYa475Zw+CXZzNr/R63SzPGFMMCwpSZ+DpVmHxjV94bnUBevnLD+4u5cdJiNqdluF2aMaYIXg0IERksIutFJFlEHi3i+AMiskZEVojITBFpUOBYnogs83xNL3yt8U8iwvkt6/Dj/f3488Ut+G3LPga9NJu/f7vGVrIzxsd4bX84iSoAABYiSURBVLI+EQkENgAXACk4S5COVNU1Bc7pDyxS1aMicjtwnqoO9xzLUNUSr1Rjk/X5p7TDWbzw43o+S9xBjUohPHRhc65OiCUwwJYbMaYsuDVZX1cgWVU3q2o2MAUYWvAEVf1FVY96NhcCMV6sx/igqCqhPDusHf+5qzeNoyrz2Jcruey1uSzanO52acZUeN4MiGhgR4HtFM++U7kJ+L7AdpiIJIrIQhG5vKgLRGSs55zEtLS0c6/YuKZNdCRTb+3BayM7cuBoNsPHL+TOT5JI2X/09BcbY7wiyO0CAETkWiAB6FdgdwNVTRWRxsDPIrJSVTcVvE5VxwPjwWliKrOCjVeICJe1r8/AlnV4Z/Ym3v51E/9ds5tb+zXhtn6NqRTiE/+5GlNhePMOIhWILbAd49l3EhEZCDwODFHVE09RqWqq5/tmYBbQ0Yu1Gh8SHhLIfQObMfPB8xjUui6vztzI+S/8yr+Xpdq0HcaUIW8GxGIgXkQaiUgIMAI4aTSSiHQE3sEJhz0F9lcXkVDP61pAL2ANpkKJrhbOayM78vltPagZEcK9U5bxp7cXsCLlgNulGVMheC0gVDUXuAuYAawFpqrqahEZJyJDPKf9C4gAPi80nLUlkCgiy4FfgGcLjn4yFUuXhjX49529eW5YO7alH2HoG/N4+PPl7Dmc6XZpxpRrtia18SuHM3N4/edkJs7bQmhQIHcNaMqYXg0JDQp0uzRj/JKtSW3KjSphwTx2cUt+vL8f3RvX4Nnv1zHopdn8uPp3658wppRZQBi/1KhWZSaM7sLkG7sSHBjA2A+XcN17v7Fh92G3SzOm3LCAMH6tb7Movr+3D09d1ooVKQe46JU5PPnvVRw4mu12acb4PQsI4/eCAwO4oVcjZj3cn1Fd4/hw4TbOe34WkxdsJdemFTfmrFlAmHKjRuUQ/nZ5G767tw+t6lXl//17NRe/Ooe5G/e6XZoxfskCwpQ7LepW5eObu/HOdZ05lpPHte8t4pbJiWxLP+J2acb4FQsIUy6JCBe2rstP9/fj/wY3Z17yXi54cTbPfr+OjKxct8szxi9YQJhyLSw4kDvOa8ovD53HZe3r8/avm+j//Cw+T9xBfr4NizWmOBYQpkKoUzWMF65uz9d39iK6WjgPf7GCy9+cx5Jt+90uzRifZQFhKpQOsdX48vaevDS8PbsPZTLsrfncN2Upuw4ec7s0Y3yOBYSpcAIChCs6xvDzg+dx94CmfLfqdwY8/yuvztxIZk6e2+UZ4zNsLiZT4e3Yd5Rnvl/Ldyt/p3aVUK7r3oCR3eKoFRHqdmnGeF1xczFZQBjjsXBzOm/O2sTsDWmEBAUwpH19xvRqSOv6kW6XZozXFBcQtkSXMR7dG9eke+OaJO85zKT5W5m2JJUvlqTQtVENxvRsyAWt6hAUaK2ypuKwOwhjTuHg0RymJu7ggwVbSdl/jOhq4VzfowEjusQRWSnY7fKMKRWuTfctIoNFZL2IJIvIo0Ucf0BE1ojIChGZKSINChwbLSIbPV+jvVmnMUWJrBTMLX0b8+vD/Xn72s7E1gjnme/X0f2ZmTz+1Uo22syxppzz2h2EiAQCG4ALgBScJUhHFlwZTkT6A4tU9aiI3A6cp6rDRaQGkAgkAAosATqr6ikHrdsdhCkLa3YeYtL8LXy9bCfZufn0ia/FmF4NOa9ZbQICxO3yjDljbt1BdAWSVXWzqmYDU4ChBU9Q1V9U9ahncyEQ43l9IfCTqu7zhMJPwGAv1mpMibSqX5Xn/tSeBY8O4OELm7Nh92FunJTIgBdm8f68LRzOzHG7RGNKjTcDIhrYUWA7xbPvVG4Cvj/La40pUzUjQrmzf1PmPjKAV0d2pEblEP76nzX0eOZn/vqf1TYxoCkXfGIUk4hci9Oc1O8MrxsLjAWIi4vzQmXGFC840BkOO6R9fZbvOMD787bw0cJtTJq/lQHNazOmVyN6Na2JiDU/Gf/jzTuIVCC2wHaMZ99JRGQg8DgwRFWzzuRaVR2vqgmqmhAVFVVqhRtzNtrHVuPlER2Z98gA7u7flGU7DnDte4u48OXZfLJoO8ey7Slt41+82UkdhNNJfT7OL/fFwChVXV3gnI7AF8BgVd1YYH8NnI7pTp5dSTid1PtO9XnWSW18TWZOHt+s2MX787aweuchIsODGdE1lut7NCS6Wrjb5RkDuPgktYhcDLwMBAITVfXvIjIOSFTV6SLyX6AtsMtzyXZVHeK59kbgz579f1fV94v7LAsI46tUlcRt+3l/3hZ+WPU7ABe2rsuYXo3o0rC6NT8ZV9lUG8b4iNQDx5i8YCtTftvBwWM5tK5flTG9GnFZ+3qEBgW6XZ6pgCwgjPExx7Lz+GppKpPmb2HD7gxqRYQwqmsc13ZvQO2qYW6XZyoQCwhjfJSqMi85nUnztzBz3R6CAoRL2tbjhl6N6BBbze3yTAVgk/UZ46NEhN7xtegdX4ute4/wwYKtfJ6YwtfLdtIxrhpjejXiojZ1CbZJAo0L7A7CGB9zODOHaUtSmDR/K1vTj1K3ahjX9WjAiC6x1LQ1KkwpsyYmY/xQfr4ya8Me3p+3lTkb9xISFMDlHeozplcjWtar6nZ5ppywJiZj/FBAgDCgRR0GtKjDxt3OGhVfJqUyNTGFbo1qMKZXIwa2rG1rVBivsTsIY/zIwaM5fJa4nQ/mbyP1wDFqRYRyeYf6XNkphlb17a7CnDlrYjKmnMnNy+eX9WlMW5LCzHW7yclTWtaryrBO0QztEE1UFeurMCVjAWFMObb/SDb/WbGTaUmpLN9xgMAAoV+zKK7sFM3AlnUIC7YH8MypWUAYU0Ek7znMtKRUvkpK5fdDmVQNC+LS9vUZ1imaTnE2rYf5XxYQxlQwefnKgk3pTEtK4YdVv3MsJ49GtSpzZcdorugUTUz1Sm6XaHyEBYQxFVhGVi7fr9zFl0mpLNicDkD3xjUY1imGi9rWIyLUBjNWZBYQxhgAUvYf5aukVL5cmsqWvUcIDw5kcJu6XNkpmp5NahFo62pXOBYQxpiTqCpJ2w8wLSmFb5bv5FBmLvUiw7i8YzTDOkXTtHYVt0s0ZcQCwhhzSpk5ecxcu4dpSSn8uiGNvHylfUwkwzrHcFm7+lSvHOJ2icaLLCCMMSWSdjiLfy9L5cukVNbsOkRwoDCgRW2GdYrhvOa1CQmyp7bLGzdXlBsMvIKzotwEVX220PG+OCvOtQNGqOoXBY7lASs9mydWmjsVCwhjStfaXYeYtsSZWXZvRhY1KocwpH19hnWKoU10VRsyW064EhAiEoizJvUFQArOmtQjVXVNgXMaAlWBh4DphQIiQ1UjSvp5FhDGeEduXj5zNu7li6QUflqzm+zcfOJrRzCscwxXdIymji1w5NfcmqyvK5Csqps9RUwBhgInAkJVt3qO5XuxDmPMOQgKDKB/i9r0b1Gbg0dz+HblLqYlpfDs9+t47od19Gpaiz91jmFQq7qEh9hT2+WJNwMiGthRYDsF6HYG14eJSCKQCzyrql8XPkFExgJjAeLi4s6hVGNMSURWCmZUtzhGdYtj694jfJmUwrSkVO6dsoyI0CAubluXYZ1i6NKwBgE2ZNbv+fITMg1UNVVEGgM/i8hKVd1U8ARVHQ+MB6eJyY0ijamoGtaqzAODmnPfwGb8tnUf05ak8O2KXUxNTCG2RjhXdIxhWKdoGtSs7Hap5ix5MyBSgdgC2zGefSWiqqme75tFZBbQEdhU7EXGmDIXECB0b1yT7o1r8tehrflx9W6mJaXw2s8beXXmRhIaVGdY5xgublOPyErBbpdrzoA3O6mDcDqpz8cJhsXAKFVdXcS5k4BvjndSi0h14KiqZolILWABMLRgB3dh1kltjG/ZdfAYXy/dybSkFJL3ZBAg0DY6kp5Na9GzSU0SGtSwPgsf4OYw14txhrEGAhNV9e8iMg5IVNXpItIF+AqoDmQCv6tqaxHpCbwD5AMBwMuq+l5xn2UBYYxvUlVWpBxk5ro9LNi0l6XbD5CbrwQHCh3jqtOrSS16Nq1J+5hq9pyFC+xBOWOMzziSlcvirftYsCmd+ZvSWbXzIKpQKSSQLg1r0LNJTXo2qUWr+lVtbqgyYAFhjPFZB4/msGBzOgs27WX+pnQ27skAIDI8mO6Na9CzidMk1bR2hD2c5wVuPQdhjDGnFVkpmMFt6jK4TV0A9hzKZMHmdOYnpzNv015mrN4NQFSVUM/dhXOHEVvD1rTwNruDMMb4tB37jjJ/017mJTtNUnszsgCIrRFOz8ZO/0WPJjWpXcWe6D4b1sRkjCkXVJXkPRnM35TOvOS9LNyczqHMXADia0c4dxdNa9G9UU0bUltCFhDGmHIpL19Zs/MQ8zz9F4u37ONYTh4i0KZ+JD2bOs1RXRpWp1KItagXxQLCGFMhZOfms2zHAeZ7AmPp9v3k5DlDajvEVjvR4d0hrhqhQfYMBlhAGGMqqKPZuSRu3c/8TenM37SXVakHyVcICw7wDKl1AqNNdGSFHVJro5iMMRVSpZAg+jaLom+zKMAZUrtoS/qJwPjnD+sAqBIWRPfGf4yQalbHhtSCBYQxpgKJrBTMoNZ1GdTaGVKbdjjLM6TWaZL6aY0zpLZm5RBa1qtK09oRNK0dQbzne82IUDfLL3PWxGSMMR479h1lweZ0Fm3ex8Y9h0nek8HR7LwTx6tXCia+dhWaFAiN+DoR1K0a5rd3HNbEZIwxJRBboxKxNSpxdYIzEXV+vrLrUCbJezLYuPswm9Iy2Lg7g+9W7uLgsZwT10WEBtEkqjJNa1c56Y4jtkYlv+7bsIAwxphTCAgQoquFE10tnH6efgxwnsfYm5FN8p4MktMySN59mOS0DOZsTGNaUsqJ80KCAmhcqzLxdarQNOqPO46GNSv7xcSEFhDGGHOGRISoKqFEVQmlR5OaJx07eCyH5D0ZbPKEx8bdh1m2Yz/frNjJ8Rb9wAChQc1KNI1yAsO566hC46jKPvW8hu9UYowx5UBkeDCdG1Snc4PqJ+0/lp3HprQM567D87Vxz2F+XreH3Pw/+oKjq4U7oVEgPJpGVXHlyXALCGOMKQPhIYG0iY6kTXTkSfuzc/PZln7EExh/hMeCTelk5eafOC+qSujJoeH5iooI9VoHuVcDQkQGA6/gLBg0QVWfLXS8L86CQu2AEcdXlPMcGw084dl8WlU/8GatxhjjhpCgAOLrVCG+ThUuKrA/L19J3X/sxGiq4+HxVVIqh7NyT5wXGR5M32ZRvDayY6nX5rWAEJFA4A3gAiAFWCwi0wstG7oduAF4qNC1NYAngQRAgSWea/d7q15jjPElgQFCXM1KxNWsxPkt65zYr6rsPpR1ookqeU8GkeHeaX7y5h1EVyBZVTcDiMgUYChwIiBUdavnWH6hay8EflLVfZ7jPwGDgU+9WK8xxvg8EaFuZBh1I8PoHV/Lq5/lzXFW0cCOAtspnn2ldq2IjBWRRBFJTEtLO+tCjTHG/C/fH4hbDFUdr6oJqpoQFRV1+guMMcaUmDcDIhWILbAd49nn7WuNMcaUAm8GxGIgXkQaiUgIMAKYXsJrZwCDRKS6iFQHBnn2GWOMKSNeCwhVzQXuwvnFvhaYqqqrRWSciAwBEJEuIpICXAW8IyKrPdfuA/6GEzKLgXHHO6yNMcaUDZvN1RhjKrDiZnP1605qY4wx3mMBYYwxpkjlpolJRNKAbefwFrWAvaVUjrf5U63gX/X6U63gX/X6U63gX/WeS60NVLXI5wTKTUCcKxFJPFU7nK/xp1rBv+r1p1rBv+r1p1rBv+r1Vq3WxGSMMaZIFhDGGGOKZAHxh/FuF3AG/KlW8K96/alW8K96/alW8K96vVKr9UEYY4wpkt1BGGOMKZIFhDHGmCJV+IAQkcEisl5EkkXkUbfrKY6ITBSRPSKyyu1aTkdEYkXkFxFZIyKrReRet2sqjoiEichvIrLcU+9f3a7pdEQkUESWisg3btdyOiKyVURWisgyEfHpOXFEpJqIfCEi60RkrYj0cLumUxGR5p5/0+Nfh0TkvlJ7/4rcB+FZFnUDBZZFBUYWWhbVZ3jW8M4AJqtqG7frKY6I1APqqWqSiFQBlgCX+/C/rQCVVTVDRIKBucC9qrrQ5dJOSUQewFmWt6qqXup2PcURka1Agqr6/INnIvIBMEdVJ3hmoq6kqgfcrut0PL/PUoFuqnouDw2fUNHvIE4si6qq2cDxZVF9kqrOBvxiVltV3aWqSZ7Xh3Fm9C3pioJlTh0Zns1gz5fP/vUkIjHAJcAEt2spT0QkEugLvAegqtn+EA4e5wObSiscwALiXJZFNSUkIg2BjsAidyspnqfJZhmwB2dNdF+u92Xg/4DC67n7KgV+FJElIjLW7WKK0QhIA973NN9NEJHKbhdVQiOAT0vzDSt6QBgvE5EIYBpwn6oecrue4qhqnqp2wFnBsKuI+GQznohcCuxR1SVu13IGeqtqJ+Ai4E5Pc6kvCgI6AW+pakfgCODTfZMAnqawIcDnpfm+FT0gbGlTL/K05U8DPlbVL92up6Q8TQq/AIPdruUUegFDPO36U4ABIvKRuyUVT1VTPd/3AF/hNO/6ohQgpcDd4xc4geHrLgKSVHV3ab5pRQ+Ic1kW1RTD0+n7HrBWVV90u57TEZEoEanmeR2OM3BhnbtVFU1VH1PVGFVtiPPf7M+qeq3LZZ2SiFT2DFTA01wzCPDJkXiq+juwQ0Sae3adD/jkwIpCRlLKzUvg3E5VWKqaKyLHl0UNBCaq6mqXyzolEfkUOA+o5Vmq9UlVfc/dqk6pF3AdsNLTrg/wZ1X9zsWailMP+MAzEiQAZ4lcnx8+6ifqAF85fzMQBHyiqj+4W1Kx7gY+9vzRuBkY43I9xfKE7gXAraX+3hV5mKsxxphTq+hNTMYYY07BAsIYY0yRLCCMMcYUyQLCGGNMkSwgjDHGFMkCwpjTEJG8QjNmltqTtSLS0B9m5zUVU4V+DsKYEjrmmYLDmArF7iCMOUueNQ6e86xz8JuINPXsbygiP4vIChGZKSJxnv11ROQrz5oTy0Wkp+etAkXkXc86FD96nuRGRO7xrKexQkSmuPRjmgrMAsKY0wsv1MQ0vMCxg6raFngdZ4ZVgNeAD1S1HfAx8Kpn/6vAr6raHmd+n+NP7ccDb6hqa+AAMMyz/1Ggo+d9bvPWD2fMqdiT1MachohkqGpEEfu3AgNUdbNnYsLfVbWmiOzFWSwpx7N/l6rWEpE0IEZVswq8R0OcqcXjPduPAMGq+rSI/ICzQNTXwNcF1qswpkzYHYQx50ZP8fpMZBV4nccffYOXAG/g3G0sFhHrMzRlygLCmHMzvMD3BZ7X83FmWQW4BpjjeT0TuB1OLE4Ueao3FZEAIFZVfwEeASKB/7mLMcab7C8SY04vvMCMtAA/qOrxoa7VRWQFzl3ASM++u3FWJHsYZ3Wy47OB3guMF5GbcO4Ubgd2neIzA4GPPCEiwKt+tPSlKSesD8KYs+Tpg0hQ1b1u12KMN1gTkzHGmCLZHYQxxpgi2R2EMcaYIllAGGOMKZIFhDHGmCJZQBhjjCmSBYQxxpgi/X93dMzouTsfogAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2C2pRPOxzqm"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DB2-EymhrSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea0a4a4a-6aea-4a76-dbda-d552a6a0aa11"
      },
      "source": [
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "test_data_labels = []\n",
        "\n",
        "for test_sentence, test_label in tqdm(zip(test_data[\"document\"], test_data[\"label\"])):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(test_sentence, MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        test_data_labels.append(test_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(test_sentence)\n",
        "        pass\n",
        "\n",
        "test_movie_input_ids = np.array(input_ids, dtype=int)\n",
        "test_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
        "test_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
        "test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_type_ids)\n",
        "test_data_labels = np.asarray(test_data_labels, dtype=np.int32) #레이블 토크나이징 리스트"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "49997it [00:11, 4229.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLu7M6TohrSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aab23f9-dd39-4408-9482-4c186e8461e4"
      },
      "source": [
        "cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size=1024)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49/49 [==============================] - 128s 3s/step - loss: 0.5349 - accuracy: 0.8552\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5348654389381409, 0.8551713228225708]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}