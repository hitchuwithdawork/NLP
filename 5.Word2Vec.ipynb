{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[HW26_Problem]Word2Vec.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "##**3. Word2Vec**\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\n",
        "2. CBOW, Skip-gram 모델을 각각 구현합니다.\n",
        "3. 모델을 실제로 학습해보고 결과를 확인합니다.\n",
        "4. 산점도를 그려 단어들의 대략적인 위치를 확인해봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utBdiiW499DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dedab2-1d7a-4b73-bb96-0b5dc40153eb"
      },
      "source": [
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 3s (3,214 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa00d6d3-0e3e-4d43-bb12-ad9b2decea37"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from konlpy.tag import Mecab,Twitter,Okt,Kkma\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "\n",
        "\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\n",
        "  \"정말 맛있습니다. 추천합니다.\",\n",
        "  \"기대했던 것보단 별로였네요.\",\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
        "]\n",
        "\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "tokenizer = Okt()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\n",
        "  tokenized = []\n",
        "  for sent in tqdm(data):\n",
        "    tokens = tokenizer.morphs(sent, stem=True)\n",
        "    tokenized.append(tokens)\n",
        "\n",
        "  return tokenized"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938e2180-521a-49d0-d3c0-5f78f9367f5e"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbd739c-28fe-4ae9-9095-3e58efcdd38a"
      },
      "source": [
        "word_count = defaultdict(int)\n",
        "\n",
        "for tokens in tqdm(train_tokenized):\n",
        "  for token in tokens:\n",
        "    word_count[token] += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 56756.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9c242d-6e86-49e0-9daa-d84841b2ecbc"
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "print(list(word_count))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8c7ad3-d27f-4f11-cf0d-5eaa16956008"
      },
      "source": [
        "w2i = {}\n",
        "for pair in tqdm(word_count):\n",
        "  if pair[0] not in w2i:\n",
        "    w2i[pair[0]] = len(w2i)\n",
        "\n",
        "i2w={v:k for k,v in w2i.items()}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 468637.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiGqiEGDL5B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9114e5-f013-4b2b-c89e-0d1ae60c6c88"
      },
      "source": [
        "print(train_tokenized)\n",
        "print(w2i)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
            "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcm_L4iJBufO"
      },
      "source": [
        "### 다음은 Word2Vec을 학습시키는 대표적인 방법인 Skipgram과 CBoW를 다룹니다. \n",
        "\n",
        "* CboW는 주변단어를 이용해, 주어진 단어를 예측하는 방법입니다.\n",
        "* Skipgram은 중심 단어를 이용하여 주변 단어를 예측하는 방법입니다.\n",
        "* 즉 데이터셋을 구성할때, input x 와 target y를 어떻게 설정하는지에 차이가 있습니다.\n",
        "\n",
        "참고자료 \n",
        "\n",
        "* https://simonezz.tistory.com/35 \n",
        "\n",
        "* https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "class CBOWDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = [] # input word\n",
        "    self.y = [] # target word\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "          ############################ ANSWER HERE ################################\n",
        "          # TODO 1: insert tokens for input self.x\n",
        "          # TODO 2: insert tokens for targets self.y\n",
        "          #########################################################################        \n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.y.append(id)\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "            ############################ ANSWER HERE ################################\n",
        "          # TODO 1: insert tokens for input self.x\n",
        "          # TODO 2: insert tokens for targets self.y\n",
        "          #########################################################################        \n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.x += [id] * 2 * window_size\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAGV5IUUba0"
      },
      "source": [
        "각 모델에 맞는 `Dataset` 객체를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e91ace-fecf-4609-e00b-fc97e072160f"
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 41901.14it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 44810.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### **모델 Class 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \n",
        "\n",
        "\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x):  # x: (B, 2W)\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x): # x: (B)\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cJalkDWYMT"
      },
      "source": [
        "두 가지 모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWUXEi8WeM-"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### **모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5053f6b8-12b7-4969-a141-b868920e5b04"
      },
      "source": [
        "cbow.train()\n",
        "cbow = cbow.to(device)\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(cbow_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = cbow(x)  # (B, V)\n",
        " \n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:04<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.687154293060303\n",
            "Train loss: 4.178494453430176\n",
            "Train loss: 5.951910018920898\n",
            "Train loss: 4.173259258270264\n",
            "Train loss: 4.435324668884277\n",
            "Train loss: 5.376952648162842\n",
            "Train loss: 4.412287712097168\n",
            "Train loss: 4.269989967346191\n",
            "Train loss: 5.516913414001465\n",
            "Train loss: 4.4279351234436035\n",
            "Train loss: 5.06522798538208\n",
            "Train loss: 4.52458381652832\n",
            "Train loss: 3.862926483154297\n",
            "Train loss: 4.40413761138916\n",
            "Train loss: 4.9259138107299805\n",
            "Train loss: 4.476088523864746\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 378.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.482848167419434\n",
            "Train loss: 4.058945655822754\n",
            "Train loss: 5.823336124420166\n",
            "Train loss: 4.048701286315918\n",
            "Train loss: 4.3056769371032715\n",
            "Train loss: 5.058187484741211\n",
            "Train loss: 4.225804328918457\n",
            "Train loss: 4.128266334533691\n",
            "Train loss: 5.37060546875\n",
            "Train loss: 4.232676982879639\n",
            "Train loss: 4.842464447021484\n",
            "Train loss: 4.151096820831299\n",
            "Train loss: 3.722926378250122\n",
            "Train loss: 4.2845988273620605\n",
            "Train loss: 4.763347148895264\n",
            "Train loss: 4.3464860916137695\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 357.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.284421920776367\n",
            "Train loss: 3.9413294792175293\n",
            "Train loss: 5.696116924285889\n",
            "Train loss: 3.926571846008301\n",
            "Train loss: 4.178072452545166\n",
            "Train loss: 4.751532077789307\n",
            "Train loss: 4.045574188232422\n",
            "Train loss: 3.9901623725891113\n",
            "Train loss: 5.232490539550781\n",
            "Train loss: 4.044889450073242\n",
            "Train loss: 4.634216785430908\n",
            "Train loss: 3.8048133850097656\n",
            "Train loss: 3.5856728553771973\n",
            "Train loss: 4.167835712432861\n",
            "Train loss: 4.602826118469238\n",
            "Train loss: 4.221523761749268\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.09135627746582\n",
            "Train loss: 3.8256583213806152\n",
            "Train loss: 5.570199012756348\n",
            "Train loss: 3.8067972660064697\n",
            "Train loss: 4.052565097808838\n",
            "Train loss: 4.458563804626465\n",
            "Train loss: 3.8711934089660645\n",
            "Train loss: 3.8555736541748047\n",
            "Train loss: 5.101768493652344\n",
            "Train loss: 3.864935874938965\n",
            "Train loss: 4.440736770629883\n",
            "Train loss: 3.488394260406494\n",
            "Train loss: 3.451173782348633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 16/16 [00:00<00:00, 365.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.053722858428955\n",
            "Train loss: 4.444342136383057\n",
            "Train loss: 4.100852012634277\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.9033312797546387\n",
            "Train loss: 3.7119431495666504\n",
            "Train loss: 5.4455389976501465\n",
            "Train loss: 3.6893320083618164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 35.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.9292120933532715\n",
            "Train loss: 4.180635929107666\n",
            "Train loss: 3.702399969100952\n",
            "Train loss: 3.724435806274414\n",
            "Train loss: 4.976904392242432\n",
            "Train loss: 3.6930065155029297\n",
            "Train loss: 4.262041091918945\n",
            "Train loss: 3.2027833461761475\n",
            "Train loss: 3.319448471069336\n",
            "Train loss: 3.942160129547119\n",
            "Train loss: 4.287937164306641\n",
            "Train loss: 3.984222412109375\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e836587-0a5e-4b57-d682-decf366ab845"
      },
      "source": [
        "skipgram.train()\n",
        "skipgram = skipgram.to(device)\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(skipgram_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = skipgram(x)  # (B, V)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 684.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.925752639770508\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 838.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.8782525062561035\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 550.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.8312647342681885\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 761.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.784806251525879\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 729.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.738893985748291\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### **테스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfcb6cf-7f42-40ca-e403-61d056c1a20b"
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = cbow.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(emb.squeeze(0))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor([-1.1359, -2.4033,  0.0648, -0.7618, -0.4681, -0.1832,  0.6468, -1.0975,\n",
            "         0.0230,  0.3510,  0.6344,  0.3349, -0.2961,  0.6960,  3.0734,  0.1408,\n",
            "        -1.7267, -1.8051, -0.2834, -1.1075, -1.8595,  0.2439,  1.5478, -0.4089,\n",
            "        -0.4088,  0.6444, -1.1826,  2.0993,  1.0611,  2.8679,  0.2395, -0.7725,\n",
            "        -0.2691, -0.2277, -0.6340, -1.3528, -0.7055,  1.9617,  0.2108,  0.2822,\n",
            "         1.5466, -0.3338, -0.3376, -2.5385, -2.4162, -0.8776, -0.2668, -1.1577,\n",
            "         0.8960,  0.4111, -0.5069, -0.3561, -0.1413, -1.2465, -0.7916,  1.0941,\n",
            "        -0.9844,  1.8533, -0.1213, -1.1095,  1.0830,  0.8932, -1.4703,  0.2053,\n",
            "        -0.1646,  0.2237,  0.3490, -0.4990, -1.9966,  0.8060, -0.6573,  0.3698,\n",
            "        -1.4315, -1.6479, -1.5489,  0.3792, -1.4551,  0.3309, -0.1099, -1.0159,\n",
            "         0.1100, -0.1257,  0.2500,  0.7006,  0.9457, -0.5268,  0.8541,  0.2981,\n",
            "         1.5260, -0.3530, -1.4279,  0.0419,  0.1560,  2.0818, -0.6871,  0.8432,\n",
            "        -0.4719,  1.0193, -1.8256,  0.7871,  0.0228, -1.8183,  0.7902,  1.9133,\n",
            "        -1.3972,  0.8018, -1.3262,  0.1585, -0.6698,  0.9686,  0.2768,  1.1426,\n",
            "        -0.7562, -0.3417, -0.2013,  0.9862, -0.8800, -0.2952, -1.7926,  2.1096,\n",
            "         2.5464, -0.0441,  1.1625, -1.1340, -1.6944,  0.4697,  0.9393,  1.5889,\n",
            "        -0.4052,  0.9059,  0.5996,  0.9302,  0.7420, -0.4239,  0.9661,  1.9020,\n",
            "        -1.1848,  0.9149, -0.3066,  0.0663,  1.0690,  0.6595, -0.1873, -0.4002,\n",
            "        -1.8334, -0.8837, -0.2802, -0.7145,  2.3225,  0.4218, -2.5117,  1.7913,\n",
            "        -1.2614, -0.8304, -1.4179,  0.1394,  0.4768,  1.0315, -0.6096,  2.0154,\n",
            "        -1.2712,  1.6306,  1.0345,  1.4490,  0.4803, -0.8460,  0.2017,  0.5594,\n",
            "         1.0609, -0.2277,  0.6085,  1.5203,  0.1355,  0.5201, -0.4659,  0.3743,\n",
            "        -0.4582,  1.0789,  1.7256, -0.2974,  0.1169,  0.0389, -0.1310, -0.1014,\n",
            "         1.2157, -0.6938, -1.3517,  0.2635, -1.4563,  0.1285, -0.9132,  0.0255,\n",
            "         1.1987,  0.2959, -1.2085,  0.6540,  1.3689, -0.8588, -1.2205,  1.2318,\n",
            "         0.5556, -0.5257, -2.3857,  0.8626, -0.8703,  0.6919,  0.0907,  0.6090,\n",
            "        -0.1721, -0.7495,  0.3842,  0.1808, -0.5857,  0.2920,  0.6137,  0.0600,\n",
            "         1.9880, -1.0842, -0.4867,  0.3405,  0.6123, -0.1824,  0.0862,  2.4179,\n",
            "        -0.0424,  0.4341,  0.9468, -0.6977, -0.1198,  0.1631,  0.3885, -1.5912,\n",
            "         0.9714,  0.5290, -1.1292,  0.6251,  0.4815,  0.0361,  0.6961,  1.3325,\n",
            "        -1.0463,  0.9152, -1.2259, -1.3971,  1.3536, -0.7243,  0.1491,  0.1792,\n",
            "         0.5404, -0.4507, -0.8133, -1.5052, -1.1518, -0.2453, -1.5035,  2.2749],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 맛\n",
            "tensor([ 2.4454e-01,  1.8393e-01,  5.2197e-01,  1.3772e+00,  8.0057e-01,\n",
            "         1.5869e+00, -1.2359e+00, -2.9377e-01, -1.2842e-01, -5.7868e-01,\n",
            "         1.3740e+00,  1.5985e-01,  9.4774e-01, -6.5127e-01, -4.5430e-01,\n",
            "         1.0077e+00, -6.3702e-01, -5.0980e-01,  3.5160e-01,  7.8440e-01,\n",
            "        -5.8457e-01,  9.6174e-01,  1.0582e+00,  1.4821e+00, -2.6861e+00,\n",
            "         6.6066e-02,  2.4775e-01, -6.6168e-02, -9.0623e-02,  2.4793e-01,\n",
            "         4.2862e-01,  1.8069e+00,  2.0572e+00,  2.3086e-01,  2.8664e-02,\n",
            "        -3.9102e-01,  1.0139e+00, -3.6918e-01, -1.0881e+00,  1.8888e+00,\n",
            "        -1.8070e+00,  2.3053e-01, -4.3762e-01, -1.2040e+00, -4.2017e-01,\n",
            "         2.8418e-01, -8.7357e-01,  2.0928e-01,  2.8858e+00, -2.0960e-01,\n",
            "        -1.5488e+00,  1.1007e+00, -1.1403e+00, -3.9259e-01, -2.8119e-01,\n",
            "        -6.3733e-01, -1.5585e+00, -5.1130e-01,  1.2620e+00,  1.0016e+00,\n",
            "         1.5261e+00, -2.8342e+00, -1.1491e+00,  6.1114e-02,  4.7433e-01,\n",
            "         5.6456e-01, -9.2954e-01,  8.2759e-01, -8.4903e-01,  1.2776e+00,\n",
            "         1.7814e-01,  1.2242e-01, -1.0711e-01,  1.1389e+00,  8.7523e-02,\n",
            "         3.9137e-02,  1.7034e-01,  4.6515e-01, -6.7887e-01,  2.7057e-01,\n",
            "         7.0647e-01,  3.0847e-01,  1.2587e+00, -1.3375e+00, -6.5098e-01,\n",
            "        -1.0745e+00,  4.6462e-01,  4.5965e-01, -9.0140e-01,  4.6252e-01,\n",
            "         7.1343e-02,  6.3333e-01,  2.2051e-01,  1.2898e+00, -8.9668e-01,\n",
            "        -1.1670e+00,  2.3510e-01,  1.9131e-01,  1.1838e-01,  4.5541e-01,\n",
            "         7.3473e-01, -1.3457e+00, -1.0931e+00,  7.8213e-01, -1.4909e+00,\n",
            "        -1.2327e+00, -7.9096e-01, -2.1986e-02,  1.8784e-01,  4.0735e-01,\n",
            "        -3.4087e-01, -1.5745e-01,  1.3792e+00, -9.9527e-01, -1.7756e-02,\n",
            "         1.8620e+00,  3.0833e-01, -1.8591e-01,  1.8584e+00,  1.1089e+00,\n",
            "        -1.4447e+00,  5.5349e-01,  7.2640e-01, -1.9713e+00,  5.3907e-01,\n",
            "        -1.0760e+00,  6.1089e-04, -1.9599e-01, -2.0116e+00, -4.6914e-01,\n",
            "        -2.0814e-01, -7.8109e-01,  5.0900e-01, -2.4269e+00, -1.0695e+00,\n",
            "        -3.1986e-01, -1.5841e+00, -9.6237e-01,  1.8144e+00,  7.1643e-01,\n",
            "        -1.6816e-01,  5.9176e-01,  1.6042e+00,  1.3941e+00,  9.8365e-01,\n",
            "         1.4310e+00, -1.4493e+00, -1.0457e-01, -6.6269e-02, -4.3488e-01,\n",
            "        -1.3086e+00,  1.0824e-01, -7.3637e-01,  1.1841e+00, -5.3386e-01,\n",
            "        -1.5589e+00, -8.9298e-01,  9.3879e-01,  6.1883e-02, -1.6562e-01,\n",
            "        -7.4948e-02,  1.3535e+00,  2.2018e-01, -3.6670e-01, -1.3434e+00,\n",
            "         8.8177e-01,  1.0090e+00, -1.2781e+00, -1.1156e+00,  2.2508e-01,\n",
            "         1.2813e+00,  2.6201e+00,  8.4580e-01, -8.2121e-01,  5.6659e-01,\n",
            "         1.8641e-03,  1.9149e+00, -8.0798e-01,  5.0247e-01,  1.1154e+00,\n",
            "        -7.7720e-01, -1.9064e+00, -1.1669e+00, -1.5143e-01, -5.6927e-01,\n",
            "        -1.2692e+00,  1.0299e+00, -5.4255e-01,  2.0849e+00,  1.2279e+00,\n",
            "         1.8174e+00,  5.4945e-01, -6.4042e-02,  1.3162e+00, -4.6932e-02,\n",
            "         1.2742e-01, -7.6995e-01, -1.8989e+00,  1.7469e+00,  4.5172e-01,\n",
            "        -2.1511e-01, -2.6642e+00,  1.3806e+00,  5.0819e-01, -3.4142e+00,\n",
            "         1.7448e+00,  6.0365e-01, -1.1953e+00,  1.1671e+00,  3.5717e-01,\n",
            "        -6.8607e-01,  6.6564e-01, -3.5366e-01, -1.4060e-01,  2.9745e-01,\n",
            "         4.1682e-01, -1.7272e-01,  4.8748e-01,  3.9120e-01, -2.4292e+00,\n",
            "         2.2501e+00,  1.9346e+00,  1.1982e+00, -1.1007e+00, -1.0176e+00,\n",
            "        -3.6188e-01,  2.9221e-01,  1.3274e+00,  4.8613e-02,  2.2800e-02,\n",
            "        -9.5385e-01,  1.8905e-01, -2.5144e+00, -1.8606e-01, -1.0160e+00,\n",
            "         9.6442e-01,  6.4809e-01, -3.9455e-01,  1.9455e+00, -1.6179e-01,\n",
            "         5.4271e-01, -1.0663e+00, -4.4157e-01, -2.0191e-01,  9.6345e-01,\n",
            "         9.1525e-01, -9.0007e-01,  5.5147e-01, -4.5894e-01,  2.7556e-01,\n",
            "        -7.2029e-01, -6.5021e-01, -7.1547e-02,  1.6056e+00, -1.7574e+00,\n",
            "         1.0516e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 서비스\n",
            "tensor([-4.5636e-01,  2.9376e+00,  7.7173e-02,  2.7522e+00,  1.1269e+00,\n",
            "        -1.5956e+00, -7.4502e-01, -3.8433e-01, -1.9932e-01, -3.9006e-01,\n",
            "        -1.0353e+00, -1.8576e-02, -4.8598e-01, -2.2905e+00,  2.3718e-01,\n",
            "        -1.3556e+00,  4.2120e-01,  1.1217e+00,  1.0817e+00,  5.3166e-01,\n",
            "         3.9245e-01, -3.9006e-01, -5.6251e-01,  1.3409e+00,  3.6470e-01,\n",
            "         3.6847e-01,  5.8107e-01,  7.7041e-01, -1.0072e+00,  1.3272e+00,\n",
            "        -1.6702e+00, -2.8788e-01,  4.9766e-01, -8.2893e-01, -7.2429e-01,\n",
            "         1.5456e+00, -6.1225e-01, -1.4431e+00, -8.7936e-01, -1.9174e+00,\n",
            "         4.2868e-01, -1.0121e+00, -1.8853e+00,  1.3824e+00,  8.8109e-01,\n",
            "         1.6578e-01, -9.3218e-01,  3.4655e-02, -4.4440e-01, -1.3522e-01,\n",
            "         4.1445e-01, -7.8580e-01,  6.0217e-01, -5.5326e-01,  2.8544e+00,\n",
            "        -8.0255e-01,  9.5015e-01, -1.3066e+00, -1.6702e-01,  1.2410e+00,\n",
            "         6.4945e-01, -1.4035e+00,  4.0352e-01,  5.0054e-01,  1.0737e+00,\n",
            "         4.1059e-01,  9.1523e-01, -4.7297e-01, -1.3641e-01, -3.7881e-01,\n",
            "         1.2090e+00, -3.6235e-03,  7.1243e-01, -5.4602e-01, -1.8125e+00,\n",
            "        -1.2313e+00, -1.3038e-01,  6.8920e-01, -6.4615e-01,  8.2379e-02,\n",
            "        -9.7581e-01,  6.9068e-02, -6.4860e-01, -1.5251e+00,  2.0030e-01,\n",
            "        -3.7496e-01,  1.9665e-01, -8.0190e-01, -2.1294e+00,  6.8981e-01,\n",
            "        -9.0290e-01, -6.0755e-01,  2.1368e-01,  1.1582e+00,  1.2113e+00,\n",
            "         3.0501e-01,  3.9778e-01,  6.7198e-01, -1.7707e-03,  6.5078e-01,\n",
            "         4.0712e-01,  9.3463e-01, -4.5956e-01,  1.0830e+00,  2.1741e-01,\n",
            "        -1.8538e+00, -6.4444e-01,  8.4472e-01,  1.2990e+00, -1.7211e+00,\n",
            "         2.0663e+00, -1.5897e+00,  7.7323e-01, -1.4347e-01,  2.1679e-01,\n",
            "        -3.3533e-01, -1.0981e+00, -9.4335e-01,  3.6748e-01, -1.7086e+00,\n",
            "         4.4905e-01, -1.2691e+00,  9.7756e-01, -2.9058e+00,  6.6270e-02,\n",
            "        -1.7576e-01, -1.8598e-01, -9.8393e-02,  1.5987e-01,  4.8233e-01,\n",
            "        -1.4098e+00,  5.4585e-02, -2.0192e+00, -9.5531e-01,  1.7816e-01,\n",
            "        -4.4019e-01, -1.0266e+00,  9.0520e-01,  7.4299e-01, -4.8449e-01,\n",
            "        -1.9755e-01,  4.3550e-01, -3.7675e-01, -1.6324e-01, -7.9212e-01,\n",
            "        -1.2588e+00, -5.0526e-01, -8.4699e-01, -5.0484e-01, -6.1455e-01,\n",
            "         1.0845e+00, -1.4119e+00, -4.2503e-01, -1.2884e+00,  1.1607e+00,\n",
            "         5.3466e-01,  4.3891e-01,  3.7077e-01, -4.0863e-01,  2.0912e+00,\n",
            "         4.8493e-02, -8.2096e-01, -3.7439e-01, -5.5362e-01,  4.8148e-01,\n",
            "        -1.1683e+00, -1.7042e-01,  8.3127e-02,  1.9000e+00, -9.4418e-01,\n",
            "         8.2282e-01, -1.4609e+00, -2.4712e-01, -8.4350e-01,  1.8072e-01,\n",
            "         1.2458e+00,  2.5297e-01,  1.9823e+00, -1.3566e+00,  7.4368e-01,\n",
            "         2.2488e+00,  1.5195e+00, -1.7830e+00, -4.2769e-01, -2.0553e+00,\n",
            "        -1.2295e+00,  5.4255e-01, -2.5992e-01,  1.0432e-01,  3.4629e-01,\n",
            "        -5.6734e-01, -7.7525e-01,  4.1073e-01, -7.8595e-01, -4.3975e-01,\n",
            "         4.0987e-01,  5.2482e-01, -1.7568e+00, -1.4084e+00, -5.5923e-01,\n",
            "        -6.7730e-01, -1.0212e+00, -3.2404e-01, -2.8455e-01,  6.2793e-01,\n",
            "         3.1795e-01,  4.1792e-01, -1.6137e+00,  5.8813e-01,  9.2387e-01,\n",
            "         4.9903e-01, -8.1852e-01, -9.7134e-01,  2.2022e+00,  2.2022e-02,\n",
            "         1.0119e+00, -7.2101e-01, -1.2994e-01,  4.3283e-01, -2.9669e+00,\n",
            "         4.6491e-01,  1.7745e+00, -5.2711e-01,  8.7494e-01, -6.5606e-01,\n",
            "        -1.7709e+00,  1.0761e+00,  2.4565e-01, -4.7946e-01, -5.0462e-01,\n",
            "        -1.5222e+00,  5.8956e-01, -1.0230e+00, -5.0248e-01, -9.6843e-01,\n",
            "        -4.4324e-01, -1.3953e+00, -9.0006e-01, -1.0665e+00,  1.2090e+00,\n",
            "         6.3466e-02, -7.1878e-01,  3.6877e-01, -7.0937e-02,  5.2546e-01,\n",
            "         7.5042e-01,  2.1279e+00,  2.9701e-01, -1.0095e-01,  1.5465e-01,\n",
            "        -1.1489e+00, -1.2889e-01,  2.2734e+00,  8.4578e-01,  8.9162e-01,\n",
            "         9.3413e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 위생\n",
            "tensor([ 1.7944, -1.2453, -0.5035,  1.2004,  0.7303, -0.2600, -1.7857,  1.3206,\n",
            "         1.4370, -0.5167,  0.1525, -0.3996, -1.5048, -2.0230, -0.3032, -0.5443,\n",
            "        -0.8811,  0.7867,  0.2468,  1.6575,  0.2203, -0.4410,  0.5218,  0.0769,\n",
            "         0.3632, -2.0817,  0.2356,  0.5011,  1.7495, -1.9352,  2.2329, -0.7640,\n",
            "        -1.0015,  1.9517,  0.0716,  0.5544, -2.4050, -1.6270, -0.2186, -1.5300,\n",
            "         0.8194, -0.0787,  0.5780, -0.6249,  0.5860, -1.0649,  0.3866, -0.0278,\n",
            "         1.0901, -0.5349,  0.1421, -0.9719, -0.4085, -2.0637,  1.0547,  1.2777,\n",
            "         0.9857,  0.7572,  0.4051, -1.5056,  0.2700, -1.0919, -0.5464,  0.3721,\n",
            "         1.8451, -0.1349,  0.1775, -0.6214,  1.2933, -1.0879, -0.8825,  0.6200,\n",
            "         1.2945, -1.4090,  0.3276, -0.0367, -0.9109, -0.8501, -0.1828,  0.4906,\n",
            "         0.3047,  3.0185, -1.2018, -0.2173,  0.1953, -0.3219,  0.3358, -0.3638,\n",
            "        -0.1776, -0.5075, -0.1783, -0.7123, -0.5146,  0.2187, -1.4447,  0.6027,\n",
            "        -0.4055,  1.1455,  2.0463,  0.3893, -0.5428, -0.5131, -1.2403, -0.3723,\n",
            "         0.7334, -2.7317,  0.6581, -1.2297, -0.3206, -0.2537,  1.4826, -1.1607,\n",
            "        -0.7433, -1.4636, -1.1973, -1.6876, -0.1615, -0.1584,  0.0082,  1.6172,\n",
            "         0.9211,  0.0461,  0.7248,  1.6039,  1.2535,  0.6805,  0.1388,  1.3393,\n",
            "         1.9132, -1.5982,  0.0568, -0.3693,  1.2103, -0.3152,  1.8896,  1.3527,\n",
            "         0.3561,  0.4949, -0.1318, -0.3064,  0.5945, -0.4413,  1.5069, -0.4339,\n",
            "         0.3944,  0.2683, -0.4185, -0.7693, -0.8078, -1.1739, -0.8673, -1.0933,\n",
            "         0.7210,  1.2625, -0.4639, -0.0311, -0.0441,  0.9296,  0.7342,  1.1616,\n",
            "        -1.5893,  0.0745,  1.3110, -0.2374, -0.9867, -1.5923, -1.1743,  1.7394,\n",
            "         0.5906,  0.7655,  0.4013, -0.5102, -1.9913, -0.7811, -1.2138, -1.1279,\n",
            "         1.3214,  1.1416,  0.8180, -1.6797, -0.1642, -0.6904,  1.0608, -1.5505,\n",
            "        -0.1988, -1.3057, -0.4388,  0.0518, -0.2451, -1.8902, -0.9783,  1.0239,\n",
            "        -0.9465,  0.9380, -0.8106,  1.0825, -0.0779,  1.2927,  0.4687,  1.2675,\n",
            "        -1.0306,  1.4654, -0.3927, -0.8836, -0.0117,  0.0238,  0.7193,  0.3099,\n",
            "        -1.2210, -1.2953,  0.1666, -0.4349,  0.4664, -0.2646, -0.4350, -0.6780,\n",
            "         0.2302,  1.2499,  0.1464, -1.9512, -0.6624,  0.1599, -0.7694, -0.6469,\n",
            "        -2.1831,  1.2314,  0.0088, -0.8846, -1.8620,  0.1652, -0.7393, -0.8612,\n",
            "         0.2901, -0.2497, -0.9079,  1.1247,  0.1574, -0.2879, -0.8968,  0.1299,\n",
            "        -2.4199, -1.1280,  0.5537,  1.8985,  0.8718, -1.3311, -0.2531, -0.4847,\n",
            "         0.4833, -0.5452, -0.5050, -1.1730, -1.6660,  0.4886, -0.9059,  0.1835],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 가격\n",
            "tensor([-0.0690,  1.1512,  0.9688,  0.0087,  0.8528,  0.3240, -0.1370,  0.5786,\n",
            "        -0.3763,  0.2394, -0.8244, -0.6262, -0.8248, -0.1989,  0.0843, -0.9507,\n",
            "        -0.3579, -2.5760,  0.2097,  0.0221,  0.0367,  1.5538,  0.5361,  0.7098,\n",
            "         0.3146, -0.0592, -0.8001, -0.5693,  0.4472, -0.1878,  0.2355,  0.0053,\n",
            "         0.7185,  1.0932,  0.8436, -0.2835,  0.0290, -1.4973,  0.4741,  0.4408,\n",
            "        -0.4655,  1.4734,  2.8016, -1.2298, -0.5362,  1.5548,  1.0839,  0.0533,\n",
            "         0.3413, -1.7574,  0.1119, -0.4577,  1.5795,  0.0097, -0.5230,  0.2566,\n",
            "        -1.8978, -0.5110, -0.3571,  0.8298, -0.2832, -0.1572, -0.0591,  0.9348,\n",
            "         0.0361, -0.9617,  1.0267, -2.3734, -1.5384,  0.1984, -0.2001,  1.5444,\n",
            "         0.2764, -2.0430, -0.5504, -0.2098, -0.0925, -0.0626, -0.5421,  0.8652,\n",
            "         0.8498, -1.1565,  0.3057, -0.4382,  1.6175, -0.1996,  1.5080, -0.5349,\n",
            "        -0.9801,  0.0449, -0.4519,  0.2561,  1.4989, -0.6849,  1.0245,  0.4589,\n",
            "         0.9157,  1.1322, -0.2731, -1.2900,  1.3492,  1.2565,  2.0120, -0.5606,\n",
            "         0.1416,  0.1621,  1.9797,  0.8621, -0.5194,  0.3514,  0.0621,  0.8571,\n",
            "         0.7903, -0.1003,  0.8254,  0.0676, -1.7348, -0.3734, -0.9190, -0.6433,\n",
            "        -1.3654,  0.3124, -0.0100,  1.8049,  0.0598, -0.3039,  0.6308, -0.8899,\n",
            "        -0.3501,  0.0950,  0.8588, -1.7536,  0.2066, -0.9868,  0.0973, -0.6922,\n",
            "         0.3621, -0.8171,  1.4986,  0.4758, -0.4341, -0.2138,  2.2389,  1.5407,\n",
            "        -0.8435, -0.6229, -0.9282, -0.2620, -0.0399,  1.4635, -1.5549,  1.3081,\n",
            "         0.3807,  2.6816, -1.0860, -1.4625, -1.7123, -0.1158, -0.4324,  0.5031,\n",
            "        -0.6343, -0.0119, -0.1547,  1.4008,  1.3077,  0.0874, -1.9606,  2.3975,\n",
            "        -0.9344,  0.4801,  0.4992,  0.5280,  0.5516,  0.1193,  0.6889,  0.6585,\n",
            "        -0.0476, -2.1120, -0.9891, -0.2184,  1.0133,  1.5366,  0.5666,  0.6373,\n",
            "         0.8677, -0.2426, -0.7858, -1.2558,  0.0097, -0.4871, -0.1650, -0.4671,\n",
            "        -0.0694, -0.3411, -0.9406,  0.1627,  1.6199,  0.5126, -1.0344, -0.3745,\n",
            "        -0.3070,  0.7389,  1.8940,  0.2038,  1.5184, -1.4968, -0.5107, -0.0694,\n",
            "         1.4817,  0.4817,  0.8364, -1.5068,  0.6320, -0.2901, -0.6601,  2.0658,\n",
            "        -0.5406,  1.3093, -0.6315, -1.1981, -1.0167,  0.4254,  0.6620, -0.1727,\n",
            "         0.5697,  0.6213, -0.6620, -0.4764,  1.0145, -0.7402,  1.0769, -1.7766,\n",
            "         1.7863,  0.0660, -1.3882, -1.7472, -0.9455,  0.1060, -0.1878, -0.7537,\n",
            "         0.2562,  0.4985,  0.4451, -0.1510, -0.2919, -0.0707,  0.1214,  0.9383,\n",
            "        -1.6256, -0.4911, -1.0034,  0.3474,  0.8715,  1.0332, -1.4451, -0.8189],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42bd1d9-c6a9-43e2-9409-803f4502b5b0"
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = skipgram.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(max(emb.squeeze(0)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor(2.6201, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 맛\n",
            "tensor(1.9594, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 서비스\n",
            "tensor(2.7926, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 위생\n",
            "tensor(3.1945, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 가격\n",
            "tensor(2.4445, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4mv-fDF29Ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc9bc17-a931-4528-9a08-2770707134f0"
      },
      "source": [
        "test_words"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['음식', '맛', '서비스', '위생', '가격']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jYY7xYd4vIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ffc2035d-9bf9-4804-e229-3559ce841bd5"
      },
      "source": [
        "i2w[25]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'다시'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OBlu8O63CXx"
      },
      "source": [
        "def most_similar(word,top_k=5):\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  input_emb = skipgram.embedding(input_id)\n",
        "  score=torch.matmul(input_emb,skipgram.embedding.weight.transpose(1,0)).view(-1)\n",
        "\n",
        "  _,top_k_ids=torch.topk(score,top_k)\n",
        "\n",
        "  return [i2w[word_id.item()] for word_id in top_k_ids][1:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c75HQoLn2_Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbafe161-88ab-47db-8756-f7bee024b534"
      },
      "source": [
        "most_similar(\"가격\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['추천', '기대하다', '저', '적']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvYEGNr2IiR"
      },
      "source": [
        "## Word2Vec 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBVrabH2IiR"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XdtYVf8Ydg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#matplotlib 패키지 한글 깨짐 처리 시작\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "#plt.rc('font', family='AppleGothic') #맥"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEypFpsw7K8q"
      },
      "source": [
        "pca=PCA(n_components=2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NAllD3y7NFo"
      },
      "source": [
        "pc_weight=pca.fit_transform(skipgram.embedding.weight.data.cpu().numpy())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KKgYYTa7Uh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84d690be-f9a8-474e-f873-88c6b7d436a1"
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for word_id,(x_coordinate,y_coordinate) in enumerate(pc_weight):\n",
        "  plt.scatter(x_coordinate,y_coordinate,color=\"blue\")\n",
        "  plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX4adG7B7qdV"
      },
      "source": [],
      "execution_count": 31,
      "outputs": []
    }
  ]
}